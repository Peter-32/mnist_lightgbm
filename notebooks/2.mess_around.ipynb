{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Install TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X, test_X = train_X / 255.0, test_X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2985 - acc: 0.9127\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1460 - acc: 0.9569\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1094 - acc: 0.9667\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0899 - acc: 0.9726\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0764 - acc: 0.9766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07290338157406077, 0.9776]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_X, train_y, epochs=5)\n",
    "\n",
    "model.evaluate(test_X,  test_y, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07290338157406077, 0.9776]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X,  test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.9% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandasql import sqldf\n",
    "q = lambda q: sqldf(q, globals())\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model, neighbors, tree, svm, ensemble\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping \n",
    "np.random.seed(3)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
    "model = Sequential()\n",
    "first_layer_size = 50\n",
    "first_hidden_layer_size = 50\n",
    "last_hidden_layer_size = 20\n",
    "hidden_layers = 2\n",
    "model.add(Dense(first_layer_size, input_dim=784, activation='relu'))\n",
    "model.add(Dense(first_hidden_layer_size, activation='relu'))\n",
    "model.add(Dense(last_hidden_layer_size, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1) \n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_X.reshape(60000,784)\n",
    "test_X = test_X.reshape(10000,784)\n",
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = keras.utils.to_categorical(train_y, 10)\n",
    "test_y = keras.utils.to_categorical(test_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit start\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 12s - loss: 0.1315 - acc: 0.9604    \n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 12s - loss: 0.1024 - acc: 0.9684    \n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 12s - loss: 0.0844 - acc: 0.9742    \n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0736 - acc: 0.9771    \n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0638 - acc: 0.9797    \n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0573 - acc: 0.9821    \n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0502 - acc: 0.9841    \n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0474 - acc: 0.9847    \n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 10s - loss: 0.0415 - acc: 0.9866    \n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0388 - acc: 0.9875    \n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0368 - acc: 0.9879    \n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0349 - acc: 0.9886    \n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0321 - acc: 0.9893    \n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0288 - acc: 0.9907    \n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0292 - acc: 0.9909    \n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0267 - acc: 0.9915    \n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0262 - acc: 0.9918    \n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0254 - acc: 0.9919    \n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0228 - acc: 0.9929    \n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0237 - acc: 0.9926    \n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0221 - acc: 0.9932    \n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0198 - acc: 0.9936    \n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0195 - acc: 0.9942    \n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0213 - acc: 0.9931    \n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0191 - acc: 0.9941    \n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0189 - acc: 0.9941    \n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0185 - acc: 0.9947    \n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0208 - acc: 0.9936    \n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 11s - loss: 0.0190 - acc: 0.9945    \n",
      "Epoch 00028: early stopping\n",
      "Fit done\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit start\")\n",
    "model.fit(train_X, train_y, epochs=50, batch_size=10, verbose=1, callbacks=[early_stop])\n",
    "print(\"Fit done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16113598644515015, 0.9746]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_y, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shorter_models = []\n",
    "for i in range(0, last_hidden_layer_size):\n",
    "    print(i)\n",
    "    new_model = Sequential()\n",
    "    new_model.add(Dense(first_layer_size, input_dim=784, activation='relu'))\n",
    "    new_model.add(Dense(first_hidden_layer_size))\n",
    "    new_model.add(Dense(1))\n",
    "    # Compile model\n",
    "    new_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for j in range(0, hidden_layers):\n",
    "        new_model.layers[j].set_weights(model.get_weights()[(2*j):(2*(j+1))])\n",
    "    new_model.layers[hidden_layers].set_weights([np.vstack(model.get_weights()[2*j][:,i]), np.array([model.get_weights()[(2*j)+1][i]])])\n",
    "    shorter_models.append(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pd.DataFrame()\n",
    "for i in range(0, last_hidden_layer_size):\n",
    "    new_train['NN_feature_{}'.format(i)] = shorter_models[i].predict(train_X).flatten()\n",
    "new_train_X = new_train.values\n",
    "\n",
    "new_test = pd.DataFrame()\n",
    "for i in range(0, last_hidden_layer_size):\n",
    "    new_test['NN_feature_{}'.format(i)] = shorter_models[i].predict(test_X).flatten()\n",
    "new_test_X = new_test.values    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X, test_X = train_X / 255.0, test_X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score using original features:  0.9727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, \\\n",
    "                            precision_score, recall_score, accuracy_score\n",
    "model.fit(train_X, train_y)\n",
    "pred_y = model.predict(test_X)\n",
    "score = accuracy_score(test_y, pred_y)\n",
    "print(\"Test score using original features: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score using new features:  0.959\n"
     ]
    }
   ],
   "source": [
    "model.fit(new_train_X, train_y)\n",
    "pred_y = model.predict(new_test_X)\n",
    "score = accuracy_score(test_y, pred_y)\n",
    "print(\"Test score using new features: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score using new and old features:  0.9736\n"
     ]
    }
   ],
   "source": [
    "model.fit(np.concatenate([new_train_X, train_X], axis=1), train_y)\n",
    "pred_y = model.predict(np.concatenate([new_test_X, test_X], axis=1))\n",
    "score = accuracy_score(test_y, pred_y)\n",
    "print(\"Test score using new and old features: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commons",
   "language": "python",
   "name": "commons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
